{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:45:56.157425Z",
     "start_time": "2025-05-09T07:45:56.141431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from typing import Union\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torcheval import metrics\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "20549961ddcd4b81",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:45:56.405620Z",
     "start_time": "2025-05-09T07:45:56.388592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_weights_pre_relu(input_dim, output_dim):\n",
    "    \"\"\" Since we're using RELU activation, we'll implement the `he` initialization.\n",
    "    I have ignored bias initialization problems, as we've got no \"real training\".\n",
    "    No consideration on imbalance etc.\n",
    "    We can test this using statistics and run few simulations to approx results with expectancy\n",
    "    \"\"\"\n",
    "    std = math.sqrt(2 / input_dim)\n",
    "    weights = torch.randn((input_dim, output_dim)) * std\n",
    "    return weights\n",
    "\n",
    "\n",
    "class SplitLinear(nn.Module):\n",
    "    def __init__(self, input_dim, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        output_dim = input_dim\n",
    "        assert input_dim % 2 == 0, f\"input_dim: {input_dim} should be even.\"\n",
    "\n",
    "        self.network = nn.Sequential(OrderedDict([\n",
    "            (\"l1\", nn.Linear(input_dim // 2, output_dim // 2)),\n",
    "            (\"a1\", nn.ReLU())\n",
    "        ]))\n",
    "        # Custom weights creation!\n",
    "        he_weights = init_weights_pre_relu(input_dim // 2, output_dim // 2)\n",
    "        he_weights.requires_grad = True\n",
    "        custom_weight = nn.Parameter(he_weights)\n",
    "        self.network.l1.weight = custom_weight\n",
    "\n",
    "    def set_verbose(self, verbose):\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.shape[1] % 2 == 0, f\"x.shape[1]: {x.shape[1]} should be even.\"\n",
    "        x1, x2 = x.split(x.shape[1] // 2, dim=-1)\n",
    "        if self.verbose:\n",
    "            print(f\"x1: {x1}\\nx2: {x2}\")\n",
    "        out1, out2 = self.network(x1), self.network(x2)\n",
    "        if self.verbose:\n",
    "            print(f\"out1: {out1}\\nout2: {out2}\")\n",
    "        return torch.cat([out1, out2], dim=-1)"
   ],
   "id": "7d7b1593639253d5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:52:40.617898Z",
     "start_time": "2025-05-09T07:52:40.590866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def q1():\n",
    "    N = 2  # Batch size\n",
    "    M = 4  # Features (1d)\n",
    "\n",
    "    model = SplitLinear(M, verbose=True)\n",
    "    x = torch.rand((N, M))\n",
    "\n",
    "    print(x)\n",
    "    y = model(x)\n",
    "    print(y)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(f\"Shapes equal: {x.shape == y.shape}\")\n",
    "\n",
    "q1()"
   ],
   "id": "95d55cad43411248",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4550, 0.0670, 0.9651, 0.1678],\n",
      "        [0.9308, 0.7598, 0.4741, 0.5297]])\n",
      "x1: tensor([[0.4550, 0.0670],\n",
      "        [0.9308, 0.7598]])\n",
      "x2: tensor([[0.9651, 0.1678],\n",
      "        [0.4741, 0.5297]])\n",
      "out1: tensor([[0., 0.],\n",
      "        [0., 0.]], grad_fn=<ReluBackward0>)\n",
      "out2: tensor([[0.1036, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.1036, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<CatBackward0>)\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "Shapes equal: True\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T07:45:56.609341Z",
     "start_time": "2025-05-09T07:45:56.586776Z"
    }
   },
   "source": [
    "class DropNorm(nn.Module):\n",
    "    def __init__(self, input_dim: Union[tuple, list, int]):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-16\n",
    "        # We init params so that y_i = x_i, similarly to batch norm\n",
    "        self.gamma = nn.Parameter(torch.ones(input_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(input_dim))\n",
    "\n",
    "    def dropout(self, x: torch.Tensor):\n",
    "        # hard set of p to 0.5 like required.\n",
    "        p = 0.5\n",
    "        if not self.training:\n",
    "            return x\n",
    "        feature_shape = x.shape[1:]\n",
    "        ele_num = math.prod(feature_shape)\n",
    "        # bitwise check for `even` num\n",
    "        assert ele_num & 1 == 0\n",
    "        half_ele = ele_num // 2\n",
    "        # The following process making sure we're dropping EXACTLY 1/2 of the `neurons`\n",
    "        # Creating tensor with half 1 and half 0\n",
    "        mask = torch.cat([torch.ones(half_ele, dtype=torch.float, device=x.device),\n",
    "                          torch.zeros(half_ele, dtype=torch.float, device=x.device)])\n",
    "        # Generate random permutation (to order the 1s and 0s) <=> shuffle\n",
    "        perm = torch.randperm(ele_num, device=x.device)\n",
    "        # Shuffle the mask, reshape to original feature shape\n",
    "        mask = mask[perm].reshape(feature_shape)\n",
    "        return x * mask / p, mask\n",
    "\n",
    "    def normalize(self, x):\n",
    "        # We want all dims EXCEPT the batch dim, to be included in the mean\n",
    "        # meaning every sample will have its own mew, sig2, and eventually norm_x.\n",
    "        dims = tuple(range(1, x.dim()))\n",
    "        mew = torch.mean(x, dtype=torch.float32, dim=dims, keepdim=True)\n",
    "        # std^2 | known also as `variance`\n",
    "        sig2 = torch.sum((x - mew) ** 2, dim=dims, keepdim=True) / math.prod(x.shape[1:])\n",
    "        norm_x = (x - mew) / torch.sqrt(sig2 + self.eps)\n",
    "        return norm_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" When training, we use dropout -> normalization and we mult with mask as requested\n",
    "            (we must multiply again with the mask, as beta might not be 0, and we want 0s)\n",
    "        When not training, we only use normalize(x)*gamma + beta.\"\"\"\n",
    "        if self.training:\n",
    "            out1, mask = self.dropout(x)\n",
    "            out2 = self.normalize(out1)\n",
    "            # We multiply at mask again because parameters that were zeroed in dropout should stay zeroed\n",
    "            out2 = (self.gamma * out2 + self.beta) * mask\n",
    "        else:\n",
    "            out2 = self.gamma * self.normalize(x) + self.beta\n",
    "        return out2\n",
    "\n",
    "\n",
    "class BasicNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv net, using natural dropout and layernorm\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape=(1, 28, 28), num_classes=10):\n",
    "        \"\"\" 28 => 14 => 7 \"\"\"\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "        self.input_shape = input_shape\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LayerNorm([32, h // 2, w // 2]),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LayerNorm([64, h // 4, w // 4]),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LayerNorm([128, h // 4, w // 4]),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((h // 4) * (w // 4) * 128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MySpecialNetwork(nn.Module):\n",
    "    \"\"\" Conv net, using custom dropout and layernorm \"\"\"\n",
    "    def __init__(self, input_shape=(1, 28, 28), num_classes=10):\n",
    "        \"\"\" 28 => 14 => 7 \"\"\"\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "        self.input_shape = input_shape\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            DropNorm([32, h // 2, w // 2]),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            DropNorm([64, h // 4, w // 4]),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            DropNorm([128, h // 4, w // 4]),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((h // 4) * (w // 4) * 128, 256),\n",
    "            nn.ReLU(),\n",
    "            DropNorm(256),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:45:56.797714Z",
     "start_time": "2025-05-09T07:45:56.785715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def norm_example():\n",
    "    x = torch.arange(0, 3 * 2 * 4).reshape(3, 2, 4)\n",
    "    print(x)\n",
    "    # We want all dims EXCEPT the batch dim, to be included in the mean\n",
    "    dims = tuple(range(1, x.dim()))\n",
    "    mew = torch.mean(x, dtype=torch.float32, dim=dims, keepdim=True)\n",
    "    sig2 = torch.sum((x - mew) ** 2, dim=dims, keepdim=True) / math.prod(x.shape[1:])\n",
    "    eps = 1e-16\n",
    "\n",
    "    norm_x = (x - mew) / torch.sqrt(sig2 + eps)\n",
    "    print(norm_x)\n",
    "    \n",
    "# Example of the norm implementation\n",
    "norm_example()"
   ],
   "id": "dcf45a51a2feb3ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]],\n",
      "\n",
      "        [[16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[[-1.5275, -1.0911, -0.6547, -0.2182],\n",
      "         [ 0.2182,  0.6547,  1.0911,  1.5275]],\n",
      "\n",
      "        [[-1.5275, -1.0911, -0.6547, -0.2182],\n",
      "         [ 0.2182,  0.6547,  1.0911,  1.5275]],\n",
      "\n",
      "        [[-1.5275, -1.0911, -0.6547, -0.2182],\n",
      "         [ 0.2182,  0.6547,  1.0911,  1.5275]]])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:45:56.985436Z",
     "start_time": "2025-05-09T07:45:56.972464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validation_loop(model, val_loader, loss_fn) -> (float, float):\n",
    "    \"\"\" validation loop copied from maman13 with few modifications \"\"\"\n",
    "    val_loss = 0.\n",
    "    metric = metrics.MulticlassAccuracy(device=DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, y)\n",
    "            metric.update(preds, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    return avg_val_loss, metric.compute()\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader,\n",
    "        loss_fn: nn.Module,\n",
    "        epochs: int = 10,\n",
    "        verbose: int = 1,\n",
    "        verbose_batch: int = 1,\n",
    "        lr: float = 1e-4,\n",
    "        wd: float = 0.05) -> nn.Module:\n",
    "    \"\"\"\n",
    "    train loop, copied from maman13 with few modifications\n",
    "\n",
    "    :param model:\n",
    "    :param train_loader:\n",
    "    :param valid_loader:\n",
    "    :param epochs:\n",
    "    :param verbose: [0, 1, 2] Level of printing information (0 None, 2 Max)\n",
    "    :param verbose_batch: if verbose is 2, how many batches before printing metrices and loss.\n",
    "    :param lr: learning rate\n",
    "    :param wd: weight decay\n",
    "    :return: a model\n",
    "    \"\"\"\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    metric = metrics.MulticlassAccuracy(device=DEVICE)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.\n",
    "        model.train()\n",
    "        metric.reset()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, y)\n",
    "            metric.update(preds, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Print every `verbose_batch` batches\n",
    "            if verbose >= 2 and i % verbose_batch == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], \"\n",
    "                      f\"Step [{i}/{len(train_loader)}], \"\n",
    "                      f\"Loss: {loss.item():.4f}\", sep=',')\n",
    "\n",
    "        # End of epoch. Run validation and print outcomes\n",
    "        avg_val_loss, metric_val = validation_loop(model, valid_loader, loss_fn)\n",
    "        if verbose >= 1:\n",
    "            print(f\"Epoch [{epoch + 1:4}/{epochs}]\", end=f\", \")\n",
    "            print(f\"trn los: {running_loss / len(train_loader):8.4f},\", f\"trn acc: {metric.compute():6.4f}\",\n",
    "                  end=', ')\n",
    "            print(f\"val loss: {avg_val_loss:8.4f}, val acc: {metric_val:6.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "4458e95d5a60d0af",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:45:57.217496Z",
     "start_time": "2025-05-09T07:45:57.164467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load data \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # mean and std from MNIST stats\n",
    "])\n",
    "train_data = datasets.MNIST('../MNIST_data', download=True, train=True, transform=transform)\n",
    "test_data = datasets.MNIST('../MNIST_data', download=True, train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=50, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=100)\n",
    "\n",
    "# Set loss fn\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "id": "739308e2a416772e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:48:59.079148Z",
     "start_time": "2025-05-09T07:45:57.365465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BasicNetwork().to(DEVICE)\n",
    "train_model(model, train_loader, test_loader, loss_fn, verbose=1, verbose_batch=100)"
   ],
   "id": "6943121dd3e1c50f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/10], trn los:   0.6348, trn acc: 0.8238, val loss:   0.1216, val acc: 0.9626\n",
      "Epoch [   2/10], trn los:   0.2371, trn acc: 0.9432, val loss:   0.0746, val acc: 0.9765\n",
      "Epoch [   3/10], trn los:   0.1894, trn acc: 0.9592, val loss:   0.0535, val acc: 0.9850\n",
      "Epoch [   4/10], trn los:   0.1812, trn acc: 0.9663, val loss:   0.0531, val acc: 0.9837\n",
      "Epoch [   5/10], trn los:   0.1905, trn acc: 0.9700, val loss:   0.0476, val acc: 0.9871\n",
      "Epoch [   6/10], trn los:   0.2167, trn acc: 0.9718, val loss:   0.0554, val acc: 0.9879\n",
      "Epoch [   7/10], trn los:   0.2575, trn acc: 0.9736, val loss:   0.0676, val acc: 0.9869\n",
      "Epoch [   8/10], trn los:   0.3001, trn acc: 0.9745, val loss:   0.0796, val acc: 0.9862\n",
      "Epoch [   9/10], trn los:   0.3108, trn acc: 0.9742, val loss:   0.0765, val acc: 0.9870\n",
      "Epoch [  10/10], trn los:   0.2993, trn acc: 0.9711, val loss:   0.0640, val acc: 0.9884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BasicNetwork(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): LayerNorm((32, 14, 14), eps=1e-05, elementwise_affine=True)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): LayerNorm((64, 7, 7), eps=1e-05, elementwise_affine=True)\n",
       "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (11): ReLU()\n",
       "    (12): Dropout(p=0.5, inplace=False)\n",
       "    (13): LayerNorm((128, 7, 7), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=6272, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:52:40.571895Z",
     "start_time": "2025-05-09T07:48:59.086149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = MySpecialNetwork().to(DEVICE)\n",
    "train_model(model, train_loader, test_loader, loss_fn, verbose=1, verbose_batch=100)"
   ],
   "id": "30e64ae744eee7b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/10], trn los:   0.6143, trn acc: 0.8345, val loss:   0.1317, val acc: 0.9580\n",
      "Epoch [   2/10], trn los:   0.2272, trn acc: 0.9473, val loss:   0.0861, val acc: 0.9737\n",
      "Epoch [   3/10], trn los:   0.1853, trn acc: 0.9608, val loss:   0.0819, val acc: 0.9747\n",
      "Epoch [   4/10], trn los:   0.1768, trn acc: 0.9680, val loss:   0.0716, val acc: 0.9778\n",
      "Epoch [   5/10], trn los:   0.1849, trn acc: 0.9728, val loss:   0.0749, val acc: 0.9787\n",
      "Epoch [   6/10], trn los:   0.2105, trn acc: 0.9737, val loss:   0.0741, val acc: 0.9817\n",
      "Epoch [   7/10], trn los:   0.2533, trn acc: 0.9747, val loss:   0.0847, val acc: 0.9824\n",
      "Epoch [   8/10], trn los:   0.2933, trn acc: 0.9757, val loss:   0.0968, val acc: 0.9830\n",
      "Epoch [   9/10], trn los:   0.3083, trn acc: 0.9750, val loss:   0.0918, val acc: 0.9824\n",
      "Epoch [  10/10], trn los:   0.2986, trn acc: 0.9749, val loss:   0.0820, val acc: 0.9844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySpecialNetwork(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): DropNorm()\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): DropNorm()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (9): ReLU()\n",
       "    (10): DropNorm()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=6272, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): DropNorm()\n",
       "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "61669841b2990e3e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
